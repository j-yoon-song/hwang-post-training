model:
  name_or_path: google/gemma-3-27b-it
  trust_remote_code: false
  attn_implementation: eager
  freeze_output_embeddings: true

data:
  train_file: ../../runs/exp001/final_dataset.jsonl
  eval_file:
  source_field: source_text
  target_field: target_text
  source_lang_name: auto
  target_lang_name: auto
  source_lang_code: en
  target_lang_code: ko
  source_lang_name_field:
  target_lang_name_field:
  source_lang_code_field:
  target_lang_code_field:
  prompt_template: |
    You are a professional {source_lang} ({src_lang_code}) to {target_lang}
    ({tgt_lang_code}) translator. Your goal is to accurately convey the meaning and
    nuances of the original {source_lang} text while adhering to {target_lang} grammar,
    vocabulary, and cultural sensitivities. Produce only the {target_lang}
    translation, without any additional explanations or commentary. Please translate
    the following {source_lang} text into {target_lang}:


    {text}
  max_train_samples:
  max_eval_samples:
  preprocessing_num_workers: 8

train:
  output_dir: ../outputs/gemma3-27b-it-sft-fsdp
  seed: 42
  num_train_epochs: 1.0
  max_steps: -1
  global_batch_size: 16
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  learning_rate: 0.0001
  warmup_ratio: 0.03
  lr_scheduler_type: cosine
  weight_decay: 0.0
  max_seq_length: 1024
  bf16: true
  tf32: true
  gradient_checkpointing: false
  dataloader_num_workers: 4
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  report_to: []
  resume_from_checkpoint:
  ddp_find_unused_parameters: false
  expected_world_size: 8
  fsdp: full_shard auto_wrap
  fsdp_transformer_layer_cls_to_wrap: auto
  fsdp_backward_prefetch: BACKWARD_PRE
  fsdp_forward_prefetch: false
  fsdp_cpu_offload: false
  fsdp_use_orig_params: true
  fsdp_limit_all_gathers: true
  fsdp_activation_checkpointing: true
  fsdp_sync_module_states: true
  fsdp_cpu_ram_efficient_loading: true
